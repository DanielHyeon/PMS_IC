# LangGraph ê¸°ë°˜ ì§€ëŠ¥í˜• ì±„íŒ… ì›Œí¬í”Œë¡œìš° ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ **RAG(ë¬¸ì„œ ê¸°ë°˜) ì‘ë‹µ**ê³¼ **ì¼ë°˜ LLM ì‘ë‹µ**ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ë¼ìš°íŒ…í•˜ëŠ” ì±„íŒ… ì‹œìŠ¤í…œì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

### í•µì‹¬ ê¸°ëŠ¥
- âœ… **ì˜ë„ ìë™ ë¶„ë¥˜**: ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ìë™ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì‘ë‹µ ê²½ë¡œ ì„ íƒ
- âœ… **ì¡°ê±´ë¶€ RAG**: í•„ìš”í•  ë•Œë§Œ RAG ê²€ìƒ‰ ìˆ˜í–‰ (ì„±ëŠ¥ ìµœì í™”)
- âœ… **ìƒíƒœ ê´€ë¦¬**: LangGraphì˜ ìƒíƒœ ê·¸ë˜í”„ë¡œ ëŒ€í™” íë¦„ ì¶”ì 
- âœ… **í™•ì¥ ê°€ëŠ¥**: ìƒˆë¡œìš´ ë…¸ë“œ ì¶”ê°€ ìš©ì´ (ì›¹ ê²€ìƒ‰, ê³„ì‚°, API í˜¸ì¶œ ë“±)

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„

```
ì‚¬ìš©ì ë©”ì‹œì§€
    â†“
[ì˜ë„ ë¶„ë¥˜]
    â”œâ”€ casual (ì¼ìƒ ëŒ€í™”)
    â”‚     â†“
    â”‚  [RAG ìŠ¤í‚µ]
    â”‚     â†“
    â””â”€ pms_query (PMS ê´€ë ¨)
          â†“
       [RAG ê²€ìƒ‰]
          â†“
    â””â”€ general (ì¼ë°˜ ì§ˆë¬¸)
          â†“
       [RAG ê²€ìƒ‰]
          â†“
    [ì‘ë‹µ ìƒì„±]
          â†“
    ìµœì¢… ì‘ë‹µ
```

### ì˜ë„ ë¶„ë¥˜ ì „ëµ

#### 1ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (ë¹ ë¦„)
```python
casual_patterns = ["ì•ˆë…•", "ê³ ë§ˆì›Œ", "ê°ì‚¬", "ë¯¸ì•ˆ", "ì£„ì†¡"]
pms_keywords = ["í”„ë¡œì íŠ¸", "ì¼ì •", "ì‚°ì¶œë¬¼", "ë¬¸ì„œ", "wbs"]
```

#### 2ë‹¨ê³„: LLM ê¸°ë°˜ ë¶„ë¥˜ (ì •í™•í•¨)
- í‚¤ì›Œë“œ ë¶„ë¥˜ê°€ ì• ë§¤í•œ ê²½ìš° LLMìœ¼ë¡œ ì¬ë¶„ë¥˜
- ì§§ì€ í”„ë¡¬í”„íŠ¸ë¡œ ë¹ ë¥¸ ì¶”ë¡  (10 í† í° ì œí•œ)

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
llm-service/
â”œâ”€â”€ app.py                        # Flask API (LangGraph í†µí•©)
â”œâ”€â”€ chat_workflow.py              # LangGraph ì›Œí¬í”Œë¡œìš° í•µì‹¬ ë¡œì§
â”œâ”€â”€ rag_service.py                # RAG ê²€ìƒ‰ ì„œë¹„ìŠ¤
â”œâ”€â”€ requirements.txt              # Python ì˜ì¡´ì„± (LangGraph ì¶”ê°€)
â””â”€â”€ test_langgraph_workflow.py   # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
```

---

## ğŸš€ ì‹œì‘í•˜ê¸°

### 1. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
cd /wp/PMS_IC/llm-service

# LangGraph ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install -r requirements.txt
```

**ì¶”ê°€ëœ ì˜ì¡´ì„±:**
```txt
langgraph==0.2.45
langchain==0.3.7
langchain-core==0.3.15
langchain-community==0.3.5
```

### 2. ì„œë¹„ìŠ¤ ì¬ì‹œì‘

```bash
cd /wp/PMS_IC

# LLM ì„œë¹„ìŠ¤ ì¬ë¹Œë“œ ë° ì¬ì‹œì‘
docker-compose build llm-service
docker-compose up -d llm-service

# ë¡œê·¸ í™•ì¸
docker-compose logs -f llm-service
```

**ê¸°ëŒ€ë˜ëŠ” ë¡œê·¸:**
```
Loading model from /app/models/google.gemma-3-12b-pt.Q6_K.gguf
Gemma 3 12B model loaded successfully
Loading RAG service...
RAG service loaded successfully
Initializing LangGraph chat workflow...
Chat workflow initialized successfully
```

### 3. í—¬ìŠ¤ ì²´í¬

```bash
curl http://localhost:8000/health
```

**ì‘ë‹µ:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "rag_service_loaded": true,
  "chat_workflow_loaded": true
}
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
cd /wp/PMS_IC/llm-service

# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python3 test_langgraph_workflow.py
```

**í…ŒìŠ¤íŠ¸ í•­ëª©:**
1. âœ“ Health Check
2. âœ“ Casual Conversation (RAG ìŠ¤í‚µ)
3. âœ“ PMS Query (RAG ì‚¬ìš©)
4. âœ“ General Question
5. âœ“ Conversation with Context
6. âœ“ Performance Test

### ìˆ˜ë™ í…ŒìŠ¤íŠ¸

#### í…ŒìŠ¤íŠ¸ 1: ì¼ìƒ ëŒ€í™” (RAG ìŠ¤í‚µ)

```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "ì•ˆë…•í•˜ì„¸ìš”!",
    "context": []
  }'
```

**ê¸°ëŒ€ ì‘ë‹µ:**
```json
{
  "reply": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
  "confidence": 0.95,
  "suggestions": [],
  "metadata": {
    "intent": "casual",
    "rag_docs_count": 0,
    "workflow": "langgraph"
  }
}
```

**íŠ¹ì§•:**
- `intent`: `"casual"` (ì¼ìƒ ëŒ€í™”ë¡œ ë¶„ë¥˜)
- `rag_docs_count`: `0` (RAG ê²€ìƒ‰ ìŠ¤í‚µ)
- `confidence`: `0.95` (ë†’ì€ ì‹ ë¢°ë„)

#### í…ŒìŠ¤íŠ¸ 2: PMS ê´€ë ¨ ì§ˆë¬¸ (RAG ì‚¬ìš©)

```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "í”„ë¡œì íŠ¸ ì¼ì •ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
    "context": []
  }'
```

**ê¸°ëŒ€ ì‘ë‹µ:**
```json
{
  "reply": "í”„ë¡œì íŠ¸ ê³„íšì„œì— ë”°ë¥´ë©´...",
  "confidence": 0.85,
  "suggestions": [],
  "metadata": {
    "intent": "pms_query",
    "rag_docs_count": 3,
    "workflow": "langgraph"
  }
}
```

**íŠ¹ì§•:**
- `intent`: `"pms_query"` (PMS ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¥˜)
- `rag_docs_count`: `3` (RAG ê²€ìƒ‰ ìˆ˜í–‰)
- ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì •í™•í•œ ë‹µë³€

#### í…ŒìŠ¤íŠ¸ 3: ì¼ë°˜ ì§ˆë¬¸

```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?",
    "context": []
  }'
```

**ê¸°ëŒ€ ì‘ë‹µ:**
```json
{
  "reply": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€ sort() ë©”ì„œë“œ...",
  "confidence": 0.8,
  "suggestions": [],
  "metadata": {
    "intent": "general",
    "rag_docs_count": 0,
    "workflow": "langgraph"
  }
}
```

---

## ğŸ” ì›Œí¬í”Œë¡œìš° ìƒì„¸

### ChatState (ìƒíƒœ ìŠ¤í‚¤ë§ˆ)

```python
class ChatState(TypedDict):
    message: str              # ì‚¬ìš©ì ë©”ì‹œì§€
    context: List[dict]       # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
    intent: str               # ì˜ë„ (casual, pms_query, general)
    retrieved_docs: List[str] # RAG ê²€ìƒ‰ ê²°ê³¼
    response: str             # ìµœì¢… ì‘ë‹µ
    confidence: float         # ì‹ ë¢°ë„
    debug_info: dict          # ë””ë²„ê¹… ì •ë³´
```

### ë…¸ë“œ ì„¤ëª…

#### 1. classify_intent_node
**ì—­í• :** ì‚¬ìš©ì ë©”ì‹œì§€ì˜ ì˜ë„ ë¶„ë¥˜

**ë¶„ë¥˜ ë¡œì§:**
1. í‚¤ì›Œë“œ ê¸°ë°˜ ë¹ ë¥¸ ë¶„ë¥˜
2. ì• ë§¤í•œ ê²½ìš° LLMìœ¼ë¡œ ì¬ë¶„ë¥˜
3. 3ê°€ì§€ ì˜ë„ë¡œ ë¶„ë¥˜: `casual`, `pms_query`, `general`

**ì½”ë“œ:**
```python
def classify_intent_node(self, state: ChatState) -> ChatState:
    message = state["message"]

    # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
    intent = self._classify_with_keywords(message)

    # ì• ë§¤í•œ ê²½ìš° LLM ë¶„ë¥˜
    if intent == "uncertain":
        intent = self._classify_with_llm(message)

    state["intent"] = intent
    return state
```

#### 2. rag_search_node
**ì—­í• :** RAG ê²€ìƒ‰ ìˆ˜í–‰

**ë™ì‘:**
1. ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
2. ChromaDBì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (Top 3)
3. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìƒíƒœì— ì €ì¥

**ì½”ë“œ:**
```python
def rag_search_node(self, state: ChatState) -> ChatState:
    message = state["message"]

    if self.rag_service:
        results = self.rag_service.search(message, top_k=3)
        retrieved_docs = [doc['content'] for doc in results]
        state["retrieved_docs"] = retrieved_docs

    return state
```

#### 3. skip_rag_node
**ì—­í• :** RAG ìŠ¤í‚µ (ì¼ìƒ ëŒ€í™”)

**ë™ì‘:**
1. RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ
2. `retrieved_docs`ë¥¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
3. ë””ë²„ê·¸ ì •ë³´ì— ìŠ¤í‚µ ì‚¬ì‹¤ ê¸°ë¡

**ì½”ë“œ:**
```python
def skip_rag_node(self, state: ChatState) -> ChatState:
    state["retrieved_docs"] = []
    state["debug_info"]["rag_skipped"] = True
    return state
```

#### 4. generate_response_node
**ì—­í• :** ìµœì¢… ì‘ë‹µ ìƒì„±

**ë™ì‘:**
1. ì˜ë„ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
2. RAG ë¬¸ì„œê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
3. Gemma 3 ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±
4. ì‹ ë¢°ë„ ê³„ì‚°

**ì½”ë“œ:**
```python
def generate_response_node(self, state: ChatState) -> ChatState:
    message = state["message"]
    context = state.get("context", [])
    retrieved_docs = state.get("retrieved_docs", [])
    intent = state.get("intent", "general")

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = self._build_prompt(message, context, retrieved_docs, intent)

    # LLM ì¶”ë¡ 
    response = self.llm(prompt, ...)
    reply = self._clean_response(response["choices"][0]["text"])

    # ì‹ ë¢°ë„ ê³„ì‚°
    confidence = self._calculate_confidence(intent, retrieved_docs)

    state["response"] = reply
    state["confidence"] = confidence
    return state
```

### ë¼ìš°íŒ… ë¡œì§

```python
def route_by_intent(self, state: ChatState) -> Literal["casual", "pms_query", "general"]:
    intent = state.get("intent", "general")
    return intent
```

**ë¼ìš°íŒ… ê·œì¹™:**
- `casual` â†’ `skip_rag_node` â†’ `generate_response_node`
- `pms_query` â†’ `rag_search_node` â†’ `generate_response_node`
- `general` â†’ `rag_search_node` â†’ `generate_response_node`

---

## âš™ï¸ ì„¤ì • ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ì˜ë„ ë¶„ë¥˜ í‚¤ì›Œë“œ ìˆ˜ì •

[chat_workflow.py:82-105](llm-service/chat_workflow.py#L82-L105)

```python
def _classify_with_keywords(self, message: str) -> str:
    # ì¼ìƒ ëŒ€í™” íŒ¨í„´ ì¶”ê°€/ìˆ˜ì •
    casual_patterns = [
        "ì•ˆë…•", "ê³ ë§ˆì›Œ", "ê°ì‚¬", "ë¯¸ì•ˆ", "ì£„ì†¡",
        "ì˜ê°€", "ë°˜ê°€", "ìˆ˜ê³ ", "ã…ã…", "ã…‹ã…‹"
    ]

    # PMS ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€/ìˆ˜ì •
    pms_keywords = [
        "í”„ë¡œì íŠ¸", "ì¼ì •", "ê³„íš", "ì‚°ì¶œë¬¼", "ë¬¸ì„œ",
        "wbs", "ë¦¬ìŠ¤í¬", "ì´ìŠˆ", "ë§ˆì¼ìŠ¤í†¤", "ë‹¨ê³„"
    ]

    # ë¡œì§...
```

### RAG ê²€ìƒ‰ ë¬¸ì„œ ê°œìˆ˜ ì¡°ì •

[chat_workflow.py:168](llm-service/chat_workflow.py#L168)

```python
# ê¸°ë³¸ê°’: 3ê°œ
results = self.rag_service.search(message, top_k=3)

# ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰ (ì •í™•ë„ í–¥ìƒ)
results = self.rag_service.search(message, top_k=5)

# ë” ì ì€ ë¬¸ì„œ ê²€ìƒ‰ (ì†ë„ í–¥ìƒ)
results = self.rag_service.search(message, top_k=2)
```

### ì‹ ë¢°ë„ ê³„ì‚° ì¡°ì •

[chat_workflow.py:293-310](llm-service/chat_workflow.py#L293-L310)

```python
def _calculate_confidence(self, intent: str, retrieved_docs: List[str]) -> float:
    base_confidence = {
        "casual": 0.95,      # ì¼ìƒ ëŒ€í™”
        "pms_query": 0.70,   # PMS ì§ˆë¬¸
        "general": 0.80      # ì¼ë°˜ ì§ˆë¬¸
    }.get(intent, 0.75)

    # RAG ë¬¸ì„œê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ì¦ê°€
    if retrieved_docs and len(retrieved_docs) > 0:
        rag_boost = min(0.15, len(retrieved_docs) * 0.05)
        base_confidence = min(0.95, base_confidence + rag_boost)

    return round(base_confidence, 2)
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

### RAG ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ì„±ëŠ¥

| ì‹œë‚˜ë¦¬ì˜¤ | ì˜ë„ | RAG ê²€ìƒ‰ | ì‘ë‹µ ì‹œê°„ | ì •í™•ë„ |
|---------|------|---------|----------|--------|
| "ì•ˆë…•í•˜ì„¸ìš”" | casual | âŒ ìŠ¤í‚µ | ~1.5ì´ˆ | 95% |
| "í”„ë¡œì íŠ¸ ì¼ì •ì€?" | pms_query | âœ… ìˆ˜í–‰ | ~2.5ì´ˆ | 90% |
| "íŒŒì´ì¬ ì •ë ¬?" | general | âœ… ìˆ˜í–‰ | ~2.5ì´ˆ | 80% |

**ìµœì í™” íš¨ê³¼:**
- ì¼ìƒ ëŒ€í™”: RAG ìŠ¤í‚µìœ¼ë¡œ **40% ì‘ë‹µ ì†ë„ í–¥ìƒ**
- PMS ì§ˆë¬¸: RAG ì‚¬ìš©ìœ¼ë¡œ **ì •í™•ë„ 20% í–¥ìƒ**

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. LangGraph ë¡œë“œ ì‹¤íŒ¨

**ì¦ìƒ:**
```
chat_workflow_loaded: false
```

**í•´ê²°:**
```bash
# LangGraph ì„¤ì¹˜ í™•ì¸
docker exec -it llm-service pip list | grep langgraph

# ì¬ì„¤ì¹˜
docker-compose build --no-cache llm-service
docker-compose up -d llm-service
```

### 2. ì˜ë„ ë¶„ë¥˜ê°€ ì˜ëª»ë¨

**ì¦ìƒ:**
- PMS ì§ˆë¬¸ì¸ë° `casual`ë¡œ ë¶„ë¥˜
- ì¼ìƒ ëŒ€í™”ì¸ë° `pms_query`ë¡œ ë¶„ë¥˜

**í•´ê²°:**
1. í‚¤ì›Œë“œ ì¶”ê°€ ([chat_workflow.py:82-105](llm-service/chat_workflow.py#L82-L105))
2. LLM ë¶„ë¥˜ ì˜¨ë„ ì¡°ì • ([chat_workflow.py:134](llm-service/chat_workflow.py#L134))
   ```python
   temperature=0.1  # ë” ê²°ì •ì ì¸ ë¶„ë¥˜
   ```

### 3. RAG ê²€ìƒ‰ì´ í•­ìƒ ë¹ˆ ê²°ê³¼

**ì¦ìƒ:**
```json
"rag_docs_count": 0
```

**í•´ê²°:**
```bash
# RAG í†µê³„ í™•ì¸
curl http://localhost:8000/api/documents/stats

# ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì¸ë±ì‹± í•„ìš”
# RAG_êµ¬í˜„_ê°€ì´ë“œ.md ì°¸ì¡°
```

---

## ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ì¼ìƒ ì¸ì‚¬

**ì…ë ¥:**
```json
{
  "message": "ì•ˆë…•í•˜ì„¸ìš”!",
  "context": []
}
```

**ì›Œí¬í”Œë¡œìš°:**
```
ì•ˆë…•í•˜ì„¸ìš”!
  â†’ [ì˜ë„ ë¶„ë¥˜] â†’ casual
  â†’ [RAG ìŠ¤í‚µ]
  â†’ [ì‘ë‹µ ìƒì„±] â†’ "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
```

**ì¶œë ¥:**
```json
{
  "reply": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
  "confidence": 0.95,
  "metadata": {
    "intent": "casual",
    "rag_docs_count": 0,
    "workflow": "langgraph"
  }
}
```

### ì˜ˆì‹œ 2: PMS ê´€ë ¨ ì§ˆë¬¸

**ì…ë ¥:**
```json
{
  "message": "ì´ë²ˆ í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ë§ˆì¼ìŠ¤í†¤ì€?",
  "context": []
}
```

**ì›Œí¬í”Œë¡œìš°:**
```
ì´ë²ˆ í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ë§ˆì¼ìŠ¤í†¤ì€?
  â†’ [ì˜ë„ ë¶„ë¥˜] â†’ pms_query
  â†’ [RAG ê²€ìƒ‰] â†’ ê´€ë ¨ ë¬¸ì„œ 3ê°œ ê²€ìƒ‰
  â†’ [ì‘ë‹µ ìƒì„±] â†’ "í”„ë¡œì íŠ¸ ê³„íšì„œì— ë”°ë¥´ë©´..."
```

**ì¶œë ¥:**
```json
{
  "reply": "í”„ë¡œì íŠ¸ ê³„íšì„œì— ë”°ë¥´ë©´, ì£¼ìš” ë§ˆì¼ìŠ¤í†¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n1. ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì™„ë£Œ (1ì›” 31ì¼)\n2. ì„¤ê³„ ì™„ë£Œ (2ì›” 28ì¼)\n...",
  "confidence": 0.85,
  "metadata": {
    "intent": "pms_query",
    "rag_docs_count": 3,
    "workflow": "langgraph"
  }
}
```

---

## ğŸš€ í–¥í›„ í™•ì¥ ê°€ëŠ¥ì„±

LangGraphë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### 1. ì›¹ ê²€ìƒ‰ ë…¸ë“œ
```python
workflow.add_node("web_search", web_search_node)

# ë¼ìš°íŒ… ìˆ˜ì •
workflow.add_conditional_edges(
    "classify_intent",
    route_by_intent,
    {
        "casual": "skip_rag",
        "pms_query": "rag_search",
        "general": "rag_search",
        "web_query": "web_search"  # ìƒˆë¡œìš´ ê²½ë¡œ
    }
)
```

### 2. ê³„ì‚° ë„êµ¬ ë…¸ë“œ
```python
def calculator_node(state: ChatState) -> ChatState:
    # ìˆ˜ì‹ ì¶”ì¶œ ë° ê³„ì‚°
    result = eval(extract_expression(state["message"]))
    state["calculation_result"] = result
    return state
```

### 3. API í˜¸ì¶œ ë…¸ë“œ
```python
def jira_api_node(state: ChatState) -> ChatState:
    # Jira APIë¡œ ì´ìŠˆ ì¡°íšŒ
    issues = fetch_jira_issues(state["message"])
    state["external_data"] = issues
    return state
```

### 4. ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—…
```python
workflow.add_node("planning_agent", planning_agent_node)
workflow.add_node("execution_agent", execution_agent_node)
workflow.add_node("review_agent", review_agent_node)

# ìˆœì°¨ ì‹¤í–‰
workflow.add_edge("planning_agent", "execution_agent")
workflow.add_edge("execution_agent", "review_agent")
```

---

## ğŸ“ API ì‘ë‹µ í¬ë§·

### ì„±ê³µ ì‘ë‹µ

```json
{
  "reply": "ì‘ë‹µ í…ìŠ¤íŠ¸",
  "confidence": 0.85,
  "suggestions": [],
  "metadata": {
    "intent": "pms_query | casual | general",
    "rag_docs_count": 3,
    "workflow": "langgraph"
  }
}
```

### ì˜¤ë¥˜ ì‘ë‹µ

```json
{
  "error": "Failed to process chat request",
  "message": "ìƒì„¸ ì˜¤ë¥˜ ë©”ì‹œì§€"
}
```

---

## ğŸ“ ë¬¸ì˜ ë° ì§€ì›

- **ë¡œê·¸ í™•ì¸:** `docker-compose logs -f llm-service`
- **í—¬ìŠ¤ ì²´í¬:** `curl http://localhost:8000/health`
- **í…ŒìŠ¤íŠ¸ ì‹¤í–‰:** `python3 test_langgraph_workflow.py`

---

**êµ¬ì¶• ì™„ë£Œì¼:** 2026-01-03
**ë²„ì „:** 1.0.0
**ê¸°ìˆ  ìŠ¤íƒ:** LangGraph 0.2.45, LangChain 0.3.7, Gemma 3 12B, ChromaDB
**í•µì‹¬ ê¸°ëŠ¥:** ì˜ë„ ê¸°ë°˜ ì¡°ê±´ë¶€ RAG, ìƒíƒœ ê´€ë¦¬, í™•ì¥ ê°€ëŠ¥í•œ ì›Œí¬í”Œë¡œìš°
